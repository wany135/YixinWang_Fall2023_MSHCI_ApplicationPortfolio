---
title: "Wang_Assignment6"
author: "Yixin Wang"
date: "2022-11-27"
output: 
  html_document: 
    toc: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Assignment 6: Sentimental Analysis**

## **Research Question**
Are AFINN scores for words predictive enough for whether a text is labeled positive or negative for Coronavirus tweets?

### **Hypothesis**
I hypothesize that the training model would express around 0.7 accuracy.

### **Road Map for SA**
1. Wrangle Data
2. Count by text for sentiment - I used AFINN, and deicde the outcome variable
3. Visualize the predictors by the outcome variable through bar charts
4. Model the data by conducting logistic regression
5. Discussion

## **Data**
The data used in this assignment was collected by Aman Miglani, ANkit Gupta, and Raenish David. The tweets all have been pulled from Twitter and manual tagging has been down by the authors. The dataset containing columns labeled as location, tweet at, original tweet, and label.

### **Variables**
Outcome Variable: Sentiment is the binary outcome variable, which is the review type divided into positive and negative.
Predictor: mean sentiment score generated by AFINN

### **Data Wrangling**
#In order to organize the data successfully the following steps were completed. 1. Called in dataframes as tibble 2. Filtered dataframes to narrow scope to answer research question 3. clean the data

```{r}
rm(list=ls(all=TRUE)) 
library(tidyverse)
library(ggplot2)
library(stringr)
library(tidytext)
```

```{r}
corona <- read.csv("Corona_NLP_train.csv") #import the data
```

```{r}
corona <- corona %>%
    na.omit()
```

Let's filter the data and only keep the variables we want to focus.
```{r}
corona_sa <- corona %>%
  select("UserName", "OriginalTweet", "Sentiment") %>%
  filter(Sentiment %in% c("Extremely Negative", "Extremely Positive", "Negative", "Positive")) %>% 
  mutate(
    Sentiment = ifelse(Sentiment == "Extremely Positive", "Positive", Sentiment), #attribute extremely positive and negative to positive and negatie
    Sentiment = ifelse(Sentiment == "Extremely Negative", "Negative", Sentiment)
  )
```

```{r}
corona_sa <- corona_sa%>%
  filter(Sentiment != "Neutral") #filter the neutral
```

```{r}
unnest_corona <- corona_sa%>% #unnest the tweets for preparing conducting afinn
  unnest_tokens(word, OriginalTweet)
```

I used AFINN to count by text for sentiment
```{r}
afinn <- get_sentiments("afinn")

afinn

#match the words from the electronics texts with afinn data

afinn_words <- unnest_corona %>%
  inner_join(afinn) 

afinn_words

#create an average sentiment score by text

mean_data_afinn <- afinn_words %>% 
  group_by(UserName) %>%
  dplyr::summarize(mean_sent = mean(value))

str(mean_data_afinn)
```

considering I have categorized the data in advance, I directly add the mean to the original dataset based on User name
```{r}
total_corona_sa <- corona_sa%>%
  right_join(mean_data_afinn, by="UserName") %>%
  mutate(Sentiment = as.factor(Sentiment))%>% #factorize the sentiment
  na.omit() #get ird of na
```

### Visualize the predictors by the outcome variable
Let's graph out some differences
```{r}
bar_plot_afinn <- total_corona_sa%>%
  ggplot(aes(x = Sentiment, y = mean_sent)) + 
  geom_bar(stat = "summary") + #you provide the mean scores.
  coord_flip() + #flip it around so it is easier to read
  xlab("Review Type") + #label x axes
  ylab("Mean sentiment") + #label y axes
  ggtitle("Mean sentiment for review types") 

bar_plot_afinn
```

### Model the data 
```{r}
set.seed(1234) #initialize a pseudorandom number generator so that train and test set will be the same each time you call functions

#create new variable in tibble for division into training and test sets
final_tib <- total_corona_sa%>% 
  mutate(id = row_number())

#70% of data as training set 
train_set <- final_tib %>% 
  sample_frac(0.70) #which to select (the 70%)


#30% of data test set 
test_set  <- anti_join(final_tib, train_set, by = 'UserName') 
#anti_join, basically says grab what is in final_tib that is not in train_set
```

Let's test if the sample sizes across both groups are equal. It looks like they are pretty equal!:)

```{r}
total_corona_sa%>%
  group_by(Sentiment)%>%
  count
```

### The Logistic Regression on training set**
```{r}
#function glm comes from base r stats package
lg_reg_sus <-glm(Sentiment ~ mean_sent, data = train_set, family = binomial)

summary(lg_reg_sus)

```

### get all the stats needed for interpreting the success of the model**
```{r}
library(rms) #lrm function to get stats from logistic regression

lg_reg_sus1 <-lrm(Sentiment ~ mean_sent, data = train_set)

lg_reg_sus1

```


Let's draw the Confusion matrix and report Kappa Values, recall, F1, and precision
```{r}
library(caret) #we need a new library called CARET. More on this later.
train_set$predicted.probabilities<-fitted(lg_reg_sus)

train_set <- train_set%>% 
  mutate(actual = ifelse(Sentiment == 'Positive', 2, 1)) %>% 
  #assign 1 to .50 and less and 2 to anything else 
  mutate(predicted = ifelse(predicted.probabilities < .50, 1, 2))

#both need to be factors
train_set$predicted <- as.factor(train_set$predicted)
train_set$actual <- as.factor(train_set$actual)
str(train_set)

confusionMatrix(train_set$actual, train_set$predicted,
                mode = "everything", #what you want to report in stats
                positive="2") 
```

Let's draw the mosaic plot for logistic regression
```{r}
#put the actual and predicted values into a table
mosaic_table <- table(train_set$actual, train_set$predicted)
mosaic_table #check on that table

#simple mosaic plot
mosaicplot(mosaic_table,
           main = "Confusion matrix for logistic regression",
           sub = "Accuracy of prediction",
           xlab = "Predicted",
           ylab = "Actual",
           color = "skyblue2",
           border = "chocolate")
```
```{r}
predicted_probability <- predict(lg_reg_sus, test_set, type = 'response') #join the test set to logistic regression

```

### Report of Kappa Values, recall, F1, and precision for final test

```{r}
test_set$pp <- predicted_probability
```

```{r}
library(caret) #we need a new library called CARET. More on this later.
train_set$pp <-fitted(lg_reg_sus)

train_set <- train_set%>% 
  mutate(actual = ifelse(Sentiment == 'Positive', 2, 1)) %>% 
  #assign 1 to .50 and less and 2 to anything else 
  mutate(predicted = ifelse(predicted.probabilities < .50, 1, 2))

#both need to be factors
train_set$predicted <- as.factor(train_set$predicted)
train_set$actual <- as.factor(train_set$actual)
str(train_set)

confusionMatrix(train_set$actual, train_set$predicted,
                mode = "everything", #what you want to report in stats
                positive="2") 
```

### draw the mosaic graph for final test 
```{r}

#put the actual and predicted values into a table
mosaic_table <- table(train_set$actual, train_set$predicted)
mosaic_table #check on that table

#simple mosaic plot
mosaicplot(mosaic_table,
           main = "Confusion matrix for logistic regression",
           sub = "Accuracy of prediction",
           xlab = "Predicted",
           ylab = "Actual",
           color = "skyblue2",
           border = "chocolate")
```

### Discussion
According to the mean sentiment bar chart, we can see that the labeled negative reports are comparatively greater than positive reports. Meanwhile, because we exclude the neutral labeled reports, we loss some objects.

The Logistic regression model of the training data showed that the R square is 15382.63, C score is 0.93, and the p-value of mean_sent is smaller than 0.0001, which is strongly significant.

Based on the results from the final test, we can see that the trained model performs pretty well on predicting test set, which had an accuracy score 0.8776. The Kappa value is 0.7546.The recall value is 0.8912, the precision value is 0.8751, and F1 value is 0.8831.

The accuracy seems not bad, based on the mosaic plot, we can see strikingly larger upper left part and bottom right cornerï¼Œwhich are considered as the right prediction. Based on the confusion matrix for final logistic gression, we can numerically see that there are 9025 objects correctly predicted as positive and 10051 correctly predicted as negative. 

The current model I built supported my hypothesized accuracy.

Due to the limitation of Sentimental Analysis, we cannot classify the tweets into more detailed positive/negative classification. 
